{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define a function to read the fvecs file format of Sift1M dataset\n",
    "def read_fvecs(fp):\n",
    "    a = np.fromfile(fp, dtype='int32')\n",
    "    d = a[0]\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy().view('float32')\n",
    "\n",
    "def read_ivecs(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predicted_neighbors, actual_neighbors):\n",
    "    total_recall = 0\n",
    "    \n",
    "    for pred, actual in zip(predicted_neighbors, actual_neighbors):\n",
    "        true_positives = len(set(pred) & set(actual))\n",
    "        possible_positives = len(set(actual))\n",
    "\n",
    "        recall = true_positives / possible_positives if possible_positives else 0\n",
    "\n",
    "        total_recall += recall\n",
    "\n",
    "    average_recall = total_recall / len(actual_neighbors)\n",
    "\n",
    "    return average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# data we will search through\n",
    "\n",
    "base = read_fvecs('./siftsmall/siftsmall_base.fvecs')  # 1M samples\n",
    "# also get some query vectors to search with\n",
    "query = read_fvecs('./siftsmall/siftsmall_query.fvecs')\n",
    "# take just one query (there are many in sift_learn.fvecs)\n",
    "# xq = xq[0].reshape(1, xq.shape[1])\n",
    "\n",
    "groundtruth = read_ivecs('./siftsmall/siftsmall_groundtruth.ivecs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Node for a navigable small world graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx : int\n",
    "        For uniquely identifying a node.\n",
    "\n",
    "    value : 1d np.ndarray\n",
    "        To access the embedding associated with this node.\n",
    "\n",
    "    neighborhood : set\n",
    "        For storing adjacent nodes.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://book.pythontips.com/en/latest/__slots__magic.html\n",
    "    https://hynek.me/articles/hashes-and-equality/\n",
    "    \"\"\"\n",
    "    __slots__ = ['idx', 'value', 'neighborhood']\n",
    "\n",
    "    def __init__(self, idx, value):\n",
    "        self.idx = idx\n",
    "        self.value = value\n",
    "        self.neighborhood = set()\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.idx)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (\n",
    "            self.__class__ == other.__class__ and\n",
    "            self.idx == other.idx\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def greedy_search(\n",
    "    graph: List[Node],\n",
    "    query: np.ndarray,\n",
    "    k: int=5,\n",
    "    m: int=50) -> Tuple[List[Tuple[float, int]], float]:\n",
    "    \"\"\"\n",
    "    Performs knn search using the navigable small world graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph :\n",
    "        Navigable small world graph from build_nsw_graph.\n",
    "\n",
    "    query : 1d np.ndarray\n",
    "        Query embedding that we wish to find the nearest neighbors.\n",
    "\n",
    "    k : int\n",
    "        Number of nearest neighbors returned.\n",
    "\n",
    "    m : int\n",
    "        The recall set will be chosen from m different entry points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The list of nearest neighbors (distance, index) tuple.\n",
    "    and the average number of hops that was made during the search.\n",
    "    \"\"\"\n",
    "    result_queue = []\n",
    "    visited_set = set()\n",
    "    \n",
    "    hops = 0\n",
    "    for _ in range(m):\n",
    "        # random entry point from all possible candidates\n",
    "        entry_node = random.randint(0, len(graph) - 1)\n",
    "        entry_dist = distance.cosine(query, graph[entry_node].value)\n",
    "        candidate_queue = []\n",
    "        heapq.heappush(candidate_queue, (entry_dist, entry_node))\n",
    "\n",
    "        temp_result_queue = []\n",
    "        while candidate_queue:\n",
    "            candidate_dist, candidate_idx = heapq.heappop(candidate_queue)\n",
    "\n",
    "            if len(result_queue) >= k:\n",
    "                # if candidate is further than the k-th element from the result,\n",
    "                # then we would break the repeat loop\n",
    "                current_k_dist, current_k_idx = heapq.nsmallest(k, result_queue)[-1]\n",
    "                if candidate_dist > current_k_dist:\n",
    "                    break\n",
    "\n",
    "            for friend_node in graph[candidate_idx].neighborhood:\n",
    "                if friend_node not in visited_set:\n",
    "                    visited_set.add(friend_node)\n",
    "\n",
    "                    friend_dist = distance.cosine(query, graph[friend_node].value)\n",
    "                    heapq.heappush(candidate_queue, (friend_dist, friend_node))\n",
    "                    heapq.heappush(temp_result_queue, (friend_dist, friend_node))\n",
    "                    hops += 1\n",
    "\n",
    "        result_queue = list(heapq.merge(result_queue, temp_result_queue))\n",
    "\n",
    "    return heapq.nsmallest(k, result_queue), hops / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    graph: List[Node],\n",
    "    query: np.ndarray,\n",
    "    k: int = 5,\n",
    "    m: int = 50,\n",
    "    beam_width: int = 10) -> Tuple[List[Tuple[float, int]], float]:\n",
    "    \"\"\"\n",
    "    Performs knn search using beam search on the navigable small world graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph :\n",
    "        Navigable small world graph from build_nsw_graph.\n",
    "\n",
    "    query : 1d np.ndarray\n",
    "        Query embedding that we wish to find the nearest neighbors.\n",
    "\n",
    "    k : int\n",
    "        Number of nearest neighbors returned.\n",
    "\n",
    "    m : int\n",
    "        The recall set will be chosen from m different entry points.\n",
    "\n",
    "    beam_width : int\n",
    "        Number of nodes to consider at each level of the search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The list of nearest neighbors (distance, index) tuple.\n",
    "    and the average number of hops that was made during the search.\n",
    "    \"\"\"\n",
    "    result_queue = []\n",
    "    visited_set = set()\n",
    "\n",
    "    hops = 0\n",
    "    for _ in range(m):\n",
    "        entry_node = random.randint(0, len(graph) - 1)\n",
    "        entry_dist = distance.cosine(query, graph[entry_node].value)\n",
    "        candidate_queue = []\n",
    "        heapq.heappush(candidate_queue, (entry_dist, entry_node))\n",
    "\n",
    "        while candidate_queue:\n",
    "            temp_result_queue = []\n",
    "            # Consider up to beam_width best candidates\n",
    "            for _ in range(min(beam_width, len(candidate_queue))):\n",
    "                candidate_dist, candidate_idx = heapq.heappop(candidate_queue)\n",
    "\n",
    "                if len(result_queue) >= k:\n",
    "                    current_k_dist, _ = heapq.nsmallest(k, result_queue)[-1]\n",
    "                    if candidate_dist > current_k_dist:\n",
    "                        break\n",
    "\n",
    "                for friend_node in graph[candidate_idx].neighborhood:\n",
    "                    if friend_node not in visited_set:\n",
    "                        visited_set.add(friend_node)\n",
    "                        friend_dist = distance.cosine(query, graph[friend_node].value)\n",
    "                        heapq.heappush(candidate_queue, (friend_dist, friend_node))\n",
    "                        heapq.heappush(temp_result_queue, (friend_dist, friend_node))\n",
    "                        hops += 1\n",
    "\n",
    "            result_queue = list(heapq.merge(result_queue, temp_result_queue))\n",
    "\n",
    "    return heapq.nsmallest(k, result_queue), hops / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nsw_graph(index_factors: np.ndarray, k: int) -> List[Node]:\n",
    "    n_nodes = index_factors.shape[0]\n",
    "    tqdm_loader = tqdm(index_factors)\n",
    "    tqdm_loader.set_description(\"Building Graph\")\n",
    "    graph = []\n",
    "    for i, value in enumerate(tqdm_loader):\n",
    "        node = Node(i, value)\n",
    "        if i > k:\n",
    "            neighbors, hops = greedy_search(graph, node.value, k)\n",
    "            neighbors_indices = [node_idx for _, node_idx in neighbors]\n",
    "        else:\n",
    "            neighbors_indices = list(range(i))\n",
    "\n",
    "        # insert bi-directional connection\n",
    "        node.neighborhood.update(neighbors_indices)\n",
    "        for i in neighbors_indices:\n",
    "            graph[i].neighborhood.add(node.idx)\n",
    "        \n",
    "        graph.append(node)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Graph: 100%|██████████| 10000/10000 [40:28<00:00,  4.12it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "# selected_rows = np.random.choice(xb.shape[0], round(0.001*xb.shape[0]), replace=False)\n",
    "# index_factors = xb[selected_rows]\n",
    "graph = build_nsw_graph(base, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"graph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph.pkl', 'rb') as f:\n",
    "    objects = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:51<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "results_greedy = []\n",
    "results_beam = []\n",
    "for q in tqdm(query):\n",
    "  g = [r[1] for r in greedy_search(graph, q, k=k)[0]]\n",
    "  b = [r[1] for r in beam_search(graph, q, k=k)[0]]\n",
    "  results_greedy.append(g)\n",
    "  results_beam.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = groundtruth[:, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9919999999999999\n"
     ]
    }
   ],
   "source": [
    "average_recall = calculate_recall(results_greedy, true)\n",
    "print(average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9059999999999995\n"
     ]
    }
   ],
   "source": [
    "average_recall = calculate_recall(results_beam, true)\n",
    "print(average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0:00:00.108000\n",
      "\n",
      "2000\n",
      "0:00:00.214994\n",
      "\n",
      "4000\n",
      "0:00:00.432016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    def time(graph):\n",
    "        start = datetime.now()\n",
    "        for _ in range(100):\n",
    "            query_point = np.random.rand(10)\n",
    "            nearest_neighbor = graph.greedy_search(query_point)\n",
    "        end = datetime.now()\n",
    "        print(len(graph.nodes))\n",
    "        print(end - start)\n",
    "        print()\n",
    "\n",
    "    def add(graph, node_count):\n",
    "        for _ in range(node_count):\n",
    "            graph.add_node(np.random.rand(10))\n",
    "\n",
    "    nsw1 = NSWGraph()\n",
    "    nsw2 = NSWGraph()\n",
    "    nsw3 = NSWGraph()\n",
    "\n",
    "    add(nsw1, 1000)\n",
    "    add(nsw2, 2000)\n",
    "    add(nsw3, 4000)\n",
    "\n",
    "    time(nsw1)\n",
    "    time(nsw2)\n",
    "    time(nsw3)\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
